<!DOCTYPE html>
<html>
  <head>
    <title>Kubernetes Introduction, Architecture et Installation </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="workshop.css">
  </head>
  <body>
    <!--
    <div style="position: absolute; left: 20%; right: 20%; top: 30%;">
      <h1 style="font-size: 3em;">Loading ...</h1>
      The slides should show up here. If they don't, it might be
      because you are accessing this file directly from your filesystem.
      It needs to be served from a web server. You can try this:
      <pre>
        docker-compose up -d
        open http://localhost:8888/workshop.html # on MacOS
        xdg-open http://localhost:8888/workshop.html # on Linux
      </pre>
      Once the slides are loaded, this notice disappears when you
      go full screen (e.g. by hitting "f").
    </div>
    -->
    <textarea id="source">class: title, self-paced

Kubernetes<br/>Introduction, Architecture et Installation<br/>

.nav[*Self-paced version*]

.debug[
```
 M slides/kube-jour1.yml
 M slides/kube-jour2.yml
 M slides/logistics.md
?? slides/common/prereqs_fr.md
?? slides/kube-jour3.yml

```

These slides have been built from commit: 838480d


[common/title-jour1.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/title-jour1.md)]
---

class: title, in-person

Kubernetes<br/>Introduction, Architecture et Installation<br/><br/></br>


.debug[[common/title-jour1.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/title-jour1.md)]
---
## Quelques infos sur le formateur

- Yiannis Georgiou - CTO Ryax Technologies

- PhD Universite Grenoble-Alpes - Gestion de ressources et orchestration sur de systemes de calcul intensif

- 11 ans a Bull/Atos Technologies - Ingenieur R&D / Architecte

.debug[[logistics.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/logistics.md)]
---

## Logistique

- La formation se d√©roulera de 9h √† 17h

- Il y aura une pause d√©jeuner √† 12h30

- N'h√©sitez pas √† interrompre pour des questions √† tout moment

- * Surtout quand vous voyez des photos de conteneurs en plein √©cran! *

.debug[[logistics.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/logistics.md)]
---

.debug[[logistics.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/logistics.md)]
---
## Versions installed

- Kubernetes 1.10.1
- Docker Engine 18.03.0-ce
- Docker Compose 1.20.1


.exercise[

- Check all installed versions:
  ```bash
  kubectl version
  docker version
  docker-compose -v
  ```

]

.debug[[kube/versions-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/versions-k8s.md)]
---

class: extra-details

## Kubernetes and Docker compatibility

- Kubernetes 1.10.x only validates Docker Engine versions [1.11.2 to 1.13.1 and 17.03.x](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.10.md#external-dependencies)

--

class: extra-details

- Are we living dangerously?

--

class: extra-details

- "Validates" = continuous integration builds

- The Docker API is versioned, and offers strong backward-compatibility

  (If a client uses e.g. API v1.25, the Docker Engine will keep behaving the same way)

.debug[[kube/versions-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/versions-k8s.md)]
---

name: toc-chapter-1

## Chapter 1

- [Pr√©-requis](#toc-pr-requis)

- [Vue d'ensemble de Docker](#toc-vue-densemble-de-docker)

- [Histoire des conteneurs ... et Docker](#toc-histoire-des-conteneurs--et-docker)

- [Our sample application](#toc-our-sample-application)

- [Identifying bottlenecks](#toc-identifying-bottlenecks)

.debug[(auto-generated TOC)]
---
name: toc-chapter-2

## Chapter 2

- [Kubernetes concepts](#toc-kubernetes-concepts)

- [Declarative vs imperative](#toc-declarative-vs-imperative)

- [Kubernetes network model](#toc-kubernetes-network-model)

- [First contact with `kubectl`](#toc-first-contact-with-kubectl)

- [Setting up Kubernetes](#toc-setting-up-kubernetes)

- [Running our first containers on Kubernetes](#toc-running-our-first-containers-on-kubernetes)

.debug[(auto-generated TOC)]
---
name: toc-chapter-3

## Chapter 3

- [The Kubernetes dashboard](#toc-the-kubernetes-dashboard)

- [Security implications of `kubectl apply`](#toc-security-implications-of-kubectl-apply)

.debug[(auto-generated TOC)]



.debug[[common/toc.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/toc.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-pr-requis
class: title

Pr√©-requis

.nav[
[Previous section](#toc-)
|
[Back to table of contents](#toc-chapter-1)
|
[Next section](#toc-vue-densemble-de-docker)
]

.debug[(automatically generated title slide)]

---
# Pr√©-requis

- Soyez √† l'aise avec la ligne de commande UNIX

  - naviguer dans les r√©pertoires

  - √©diter des fichiers

  - un peu de bash-fu (variables d'environnement, boucles)

- Quelques connaissances de Docker

  - `docker run`,` docker ps`, `docker build`

  - id√©alement, vous savez √©crire un Dockerfile et le construire
    <br/>
    (m√™me si c'est une ligne `FROM` et quelques commandes` RUN`)

- C'est tout √† fait OK si vous n'√™tes pas un expert Docker!

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/prereqs_fr.md)]
---

class: title

*Dites-moi et j'oublie.*
<br/>
*Apprends-moi et je me souviens.*
<br/>
*Implique-moi et j'apprends.*

Misattribu√© √† Benjamin Franklin

[(Probablement inspir√© par le philosophe confuc√©en chinois Xunzi)](https://www.barrypopik.com/index.php/new_york_city/entry/tell_me_and_i_forget_teach_me_and_i_may_remember_involve_me_and_i_will_lear/)

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/prereqs_fr.md)]
---

## Sections pratiques

- Tout l'atelier est pratique

- Nous allons construire, exp√©dier et faire fonctionner des conteneurs!

- Vous √™tes invit√©s √† reproduire toutes les d√©mos

- Toutes les sections pratiques sont clairement identifi√©es, comme le rectangle gris ci-dessous

.exercise[

- C'est ce que tu es cens√© faire!

- Allez dans [container.training] (http://container.training/) pour voir ces diapositives

]

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/prereqs_fr.md)]
---

class: in person

## O√π allons-nous faire fonctionner nos conteneurs?

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/prereqs_fr.md)]
---

class: in person, pic

![Vous obtenez un cluster](images/you-get-a-cluster.jpg)

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/prereqs_fr.md)]
---

class: in person

## Vous obtenez un cluster de machines virtuelles cloud

- Chaque personne re√ßoit un cluster priv√© de machines virtuelles cloud (non partag√©es avec d'autres utilisateurs)

- Ils resteront la pendant la dur√©e de la formation

- Vous pouvez automatiquement SSH d'une VM √† l'autre

- Les n≈ìuds ont des alias: `asterix-1`,` asterix-2`, etc *ou* `obelix-1`,` obelix-2`, etc *ou* etc

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/prereqs_fr.md)]
---

class: in person

## Pourquoi ne faisons-nous pas des conteneurs localement?

- L'installation de ce truc peut √™tre difficile sur certaines machines

  (32 bits CPU ou OS ... Ordinateurs portables sans acc√®s administrateur ... etc.)

- *"Toute l'√©quipe a t√©l√©charg√© toutes ces images de conteneurs √† partir du WiFi!
  <br/> ... et √ßa s'est bien pass√©! "* (Litt√©ralement, personne ne l'a jamais fait)

- Tout ce dont vous avez besoin est un ordinateur (ou m√™me un t√©l√©phone ou une tablette!), Avec:

  - une connexion internet

  - un navigateur Web

  - un client SSH

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/prereqs_fr.md)]
---

class: in person

## Connexion √† l'environnement d'exercices

.exercise[

- Connectez-vous √† la premi√®re machine virtuelle (`asterix-1`) avec votre client SSH

- V√©rifiez que vous pouvez SSH (sans mot de passe) √† `asterix-2`:
  ```bash
  ssh asterix-2
  ```
- Tapez `exit` ou` ^ D` pour revenir √† `asterix-1`

]

Si quelque chose ne va pas, demandez de l'aide!

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/prereqs_fr.md)]
---

## Faire ou refaire des exercises seul?

- Utilisez quelque chose comme
  [Play-With-Docker](http://play-with-docker.com/) ou
  [Play-With-Kubernetes](https://medium.com/@marcosnils/introducing-pwk-play-with-k8s-159fcfeb787b)

  Z√©ro effort d'installation; mais l'environnement est de courte dur√©e et
  pourrait avoir des ressources limit√©es

- Cr√©ez votre propre cluster (VM locales ou cloud)

  Petit effort d'installation; petit co√ªt; environnements flexibles

- Cr√©er un tas de clusters pour vous et vos amis
    ([instructions](https://github.com/jpetazzo/container.training/tree/master/prepare-vms))

  Effort de configuration plus important; id√©al pour la formation de groupe

.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/prereqs_fr.md)]
---

## Nous allons (surtout) interagir avec asterix-1 seulement

*Ces remarques ne s'appliquent que lorsque vous utilisez plusieurs n≈ìuds, bien s√ªr.*

- Sauf instructions, **toutes les commandes doivent √™tre ex√©cut√©es √† partir de la premi√®re VM, `asterix-1`**

- Nous allons seulement v√©rifier / copier le code sur `asterix-1`

- Pendant les op√©rations normales, nous n'avons pas besoin d'acc√©der aux autres n≈ìuds

- Si nous devions r√©soudre les probl√®mes, nous utiliserions une combinaison de:

  - SSH (pour acc√©der aux logs du syst√®me, √©tat du d√©mon ...)
  
  - API Docker (pour v√©rifier l'√©tat des conteneurs en cours d'ex√©cution et du moteur de conteneur)


.debug[[common/prereqs_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/prereqs_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-vue-densemble-de-docker
class: title

Vue d'ensemble de Docker

.nav[
[Previous section](#toc-pr-requis)
|
[Back to table of contents](#toc-chapter-1)
|
[Next section](#toc-histoire-des-conteneurs--et-docker)
]

.debug[(automatically generated title slide)]

---
# Vue d'ensemble de Docker

Dans cette partie, nous allons apprendre:

* Pourquoi les conteneurs ('elevator pitch' non-technique)

* Pourquoi les conteneurs ('elevator pitch' technique)

* Comment Docker nous aide √† construire, exp√©dier et ex√©cuter

* L'histoire des conteneurs

Nous n'utiliserons pas Docker ni les conteneurs dans ce chapitre (pour l'instant!).

Ne vous inqui√©tez pas, nous y arriverons assez vite!

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

## Elevator pitch

### (pour votre manager, votre patron ...)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

## OK ... Pourquoi le buzz autour des conteneurs?

* L'industrie du logiciel a chang√©

* Avant:
  * applications monolithiques
  * longs cycles de d√©veloppement
  * environnement unique
  * redimensionner lentement

* √Ä pr√©sent:
  * services d√©coupl√©s
  * am√©liorations rapides et it√©ratives
  * plusieurs environnements
  * √âlargir rapidement

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

## Le d√©ploiement devient tr√®s complexe

* Beaucoup de piles diff√©rentes:
  * langues
  * cadres
  * des bases

* Beaucoup de cibles diff√©rentes:
  * environnements de d√©veloppement individuels
  * pr√©-production, QA, mise en sc√®ne ...
  * production: sur pr√©m, nuage, hybride

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: pic

## Le probl√®me de d√©ploiement

![problem](images/shipping-software-problem.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: pic

## La matrice de l'enfer

![matrix](images/shipping-matrix-from-hell.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: pic

## Le parall√®le avec l'industrie maritime

![history](images/shipping-industry-problem.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: pic

## Conteneurs d'exp√©dition intermodaux

![shipping](images/shipping-industry-solution.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: pic

## Un nouvel √©cosyst√®me d'exp√©dition

![shipeco](images/shipping-indsutry-results.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: pic

## Un syst√®me de conteneur d'exp√©dition pour les applications

![shipapp](images/shipping-software-solution.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: pic

## √âliminer la matrice de l'enfer

![elimatrix](images/shipping-matrix-solved.png)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

## R√©sultats

* [Dev-to-prod r√©duit de 9 mois √† 15 minutes (ING)](
  https://www.docker.com/sites/default/files/CS_ING_01.25.2015_1.pdf)

* [Temps d'int√©gration continue r√©duit de plus de 60% (BBC)](
  https://www.docker.com/sites/default/files/CS_BBCNews_01.25.2015_1.pdf)

* [D√©ployer 100 fois par jour au lieu d'une fois par semaine (GILT)](
  https://www.docker.com/sites/default/files/CS_Gilt%20Groupe_03.18.2015_0.pdf)

* [Consolidation de l'infrastructure de 70% (MetLife)](
  https://www.docker.com/customers/metlife-transforms-customer-experience-legacy-and-microservices-mashup)

* [Consolidation de l'infrastructure de 60% (Intesa Sanpaolo)](
  https://blog.docker.com/2017/11/intesa-sanpaolo-builds-resilient-foundation-banking-docker-enterprise-edition/)

* [14x densit√© d'application; 60% du centre de donn√©es existant migr√© en 4 mois (GE Appliances)](
  https://www.docker.com/customers/ge-uses-docker-enable-self-service-their-developers)

* etc.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

## Elevator pitch

###(pour vos coll√®gues devs et ops)

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

## √âchapper √† la d√©pendance d'enfer

1. √âcrire les instructions d'installation dans un fichier `INSTALL.txt`

2. En utilisant ce fichier, √©crivez un script `install.sh` qui *vous convient*

3. Transformez ce fichier dans un `Dockerfile`, testez-le sur votre machine

4. Si le Dockerfile se construit sur votre machine, il se construira *n'importe o√π*

5. R√©jouis-toi en √©vitant l'enfer de la d√©pendance et "ca marche sur ma machine"

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

## D√©veloppeurs et contributeurs embarqu√©s rapidement

1. √âcrire des fichiers Docker pour vos composants d'application

2. Utilisez des images pr√©-faites depuis le Docker Hub (mysql, redis ...)

3. D√©crivez votre pile avec un fichier Compose

4. Embarquez quelqu'un avec deux commandes:

```bash
git clone ...
docker-compose up
```

Avec cela, vous pouvez cr√©er des environnements de d√©veloppement, d'int√©gration et d'assurance qualit√© en quelques minutes!

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: extra-details

## Mettre en ≈ìuvre un CI fiable facilement

1. Construire un environnement de test avec un fichier Dockerfile ou Compose

2. Pour chaque s√©rie de tests, placez un nouveau conteneur ou une nouvelle pile

3. Chaque course est maintenant dans un environnement propre

4. Aucune pollution des tests pr√©c√©dents

Beaucoup plus rapide et moins cher que de cr√©er des machines virtuelles √† chaque fois!

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: extra-details

## Utiliser les images de conteneur comme artefacts de construction

1. Construisez votre application depuis Dockerfiles

2. Stocker les images r√©sultantes dans un registre

3. Garde-les pour toujours (ou aussi longtemps que n√©cessaire)

4. Testez ces images dans QA, CI, int√©gration ...

5. Ex√©cuter les m√™mes images en production

6. Quelque chose ne va pas? Retour √† l'image pr√©c√©dente

7. Enqu√™ter sur l'ancienne r√©gression? L'ancienne image vous couvre!

Les images contiennent toutes les biblioth√®ques, d√©pendances, etc. n√©cessaires pour ex√©cuter l'application.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: extra-details

## D√©coupler "plomberie" de la logique de l'application

1. Ecrivez votre code pour vous connecter aux services nomm√©s ("db", "api" ...)

2. Utilisez Composer pour commencer votre pile

3. Docker va configurer le r√©solveur DNS par conteneur pour ces noms

4. Vous pouvez maintenant redimensionner, ajouter des √©quilibreurs de charge, r√©plication ... sans changer votre code

Note: ceci n'est pas couvert dans cet atelier de niveau intro!

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: extra-details

## Qu'est-ce que Docker a apport√© √† la table?

### Docker avant / apr√®s

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: extra-details

## Formats et API, avant Docker

* Pas de format d'√©change standardis√©.
  <br/>(Non, une archive tar de rootfs *n'est pas* un format!)

* Les conteneurs sont difficiles √† utiliser pour les d√©veloppeurs.
  <br/>(O√π est l'√©quivalent de `docker run debian`?)

* En cons√©quence, ils sont cach√©s aux utilisateurs finaux.

* Aucun composant, API ou outil r√©utilisable.
  <br/>(Au mieux: abstractions de VM, par exemple libvirt.)


Analogie:

* Les conteneurs d'exp√©dition ne sont pas seulement des bo√Ætes en acier.
* Ce sont des bo√Ætes en acier de taille standard, avec les m√™mes crochets et trous.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: extra-details

## Formats et API, apr√®s Docker

* Normaliser le format du conteneur, car les conteneurs n'√©taient pas portables.

* Rendre les conteneurs faciles √† utiliser pour les d√©veloppeurs.

* Accent sur les composants r√©utilisables, API, √©cosyst√®me d'outils standards.

* Am√©lioration des outils sp√©cifiques ad-hoc, internes.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: extra-details

## Exp√©dition, avant Docker

* Exp√©dier les paquets: deb, rpm, gem, pot, homebrew ...

* D√©pendance de l'enfer.

* "Fonctionne sur ma machine."

* D√©ploiement de base souvent fait √† partir de z√©ro (debootstrap ...) et peu fiable.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: extra-details

## Exp√©dition, apr√®s Docker

* Exp√©dier des images de conteneur avec toutes leurs d√©pendances.

* Les images sont plus grandes, mais elles sont divis√©es en couches.

* Envoyez uniquement les couches qui ont chang√©.

* Enregistrer l'utilisation du disque, du r√©seau, de la m√©moire.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: extra-details

## Exemple

Couches:

* CentOS
* JRE
* Matou
* D√©pendances
* Application JAR
* Configuration

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: extra-details

## Devs vs Ops, avant Docker

* D√©posez une archive tar (ou un hash de commit) avec des instructions.

* Environnement de d√©veloppement tr√®s diff√©rent de la production.

* Les Ops n'ont pas toujours un environnement de dev eux-m√™mes ...

* ... et quand ils le font, cela peut diff√©rer de ceux des d√©veloppeurs.

* Les op√©rations doivent trier les diff√©rences et le faire fonctionner ...

* ... ou rebondir vers les d√©veloppeurs.

* Le code de livraison provoque des frictions et des retards.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: extra-details

## Devs vs Ops, apr√®s Docker

* D√©poser une image de conteneur ou un fichier de composition.

* Les op√©rations peuvent toujours ex√©cuter cette image de conteneur.

* Les op√©rations peuvent toujours ex√©cuter ce fichier de composition.

* Les op√©rations doivent encore s'adapter √† l'environnement de prod,
   mais au moins ils ont un point de r√©f√©rence.

* Les op√©rations ont des outils permettant d'utiliser la m√™me image
   en dev et prod.

* Les d√©veloppeurs peuvent √™tre autoris√©s √† faire eux-m√™mes des publications
   plus facilement.

.debug[[intro/Docker_Overview_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_Overview_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-histoire-des-conteneurs--et-docker
class: title

Histoire des conteneurs ... et Docker

.nav[
[Previous section](#toc-vue-densemble-de-docker)
|
[Back to table of contents](#toc-chapter-1)
|
[Next section](#toc-our-sample-application)
]

.debug[(automatically generated title slide)]

---
# Histoire des conteneurs ... et Docker

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

## Premi√®res exp√©rimentations

* [IBM VM/370 (1972)](https://en.wikipedia.org/wiki/VM_%28operating_system%29)

* [Linux VServers (2001)](http://www.solucorp.qc.ca/changes.hc?projet=vserver)

* [Solaris Containers (2004)](https://en.wikipedia.org/wiki/Solaris_Containers)

* [FreeBSD jails (1999)](https://www.freebsd.org/cgi/man.cgi?query=jail&sektion=8&manpath=FreeBSD+4.0-RELEASE)

Les conteneurs existent depuis *tr√®s longtemps* en effet.

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

class: pic

## L'√¢ge du VPS (jusqu'en 2007-2008)

![lightcont](images/containers-as-lightweight-vms.png)

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

## Containers = moins cher que les VM

* Utilisateurs: fournisseurs d'h√©bergement.

* Audience hautement sp√©cialis√©e avec une forte culture d'op√©rations.

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

class: pic

## La p√©riode PAAS (2008-2013)

![heroku 2007](images/heroku-first-homepage.png)

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

## Containers = plus facile que les VM

* Je ne peux pas parler pour Heroku, mais les conteneurs √©taient (l'une des) arme secr√®te de dotCloud

* dotCloud utilisait un PaaS, en utilisant un moteur de conteneur personnalis√©.

* Ce moteur √©tait bas√© sur OpenVZ (et plus tard, LXC) et AUFS.

* Il a commenc√© (vers 2008) comme un seul script Python.

* En 2012, le moteur avait plusieurs (10) composants Python.
  <br/> (et ~ 100 autres micro-services!)

* Fin 2012, dotCloud refactorise ce moteur de conteneur.

* Le nom de code de ce projet est "Docker".

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

## Premi√®re version publique de Docker

* Mars 2013, PyCon, Santa Clara:
  <br/> "Docker" est pr√©sent√© au public pour la premi√®re fois.

* Il est publi√© avec une licence open source.

* R√©actions et retours tr√®s positifs!

* L'√©quipe dotCloud passe progressivement au d√©veloppement de Docker.

* La m√™me ann√©e, dotCloud change de nom pour Docker.

* En 2014, l'activit√© PaaS est vendue.

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

## Docker premiers jours (2013-2014)

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

## Premiers utilisateurs de Docker

* Constructeurs PAAS (Flynn, Dokku, Tsuru, Deis ...)

* Utilisateurs de PAAS (ceux qui sont assez grands pour justifier la construction de leurs propres)

* Plates-formes CI

* d√©veloppeurs, d√©veloppeurs, d√©veloppeurs, d√©veloppeurs

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

## Boucle de r√©troaction positive

* En 2013, la technologie sous conteneurs (cgroups, namespaces, stockage copy-on-write ...)
  avait beaucoup de taches aveugles.

* La popularit√© croissante de Docker et des conteneurs a r√©v√©l√© de nombreux bugs.

* En cons√©quence, ces bugs ont √©t√© corrig√©s, ce qui a permis d'am√©liorer la stabilit√© des conteneurs.

* Tout h√©bergeur / fournisseur de cloud d√©cent peut ex√©cuter des conteneurs aujourd'hui.

* Les conteneurs deviennent un excellent outil pour d√©ployer / d√©placer des charges de travail de / sur le site / cloud.

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

## Maturit√© (2015-2016)

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

## Docker devient un standard de l'industrie

* Docker atteint le jalon symbolique 1.0.

* Les syst√®mes existants tels que Mesos et Cloud Foundry ajoutent un support Docker.

* Normalisation autour de l'OCI (Open Containers Initiative).

* D'autres moteurs de conteneurs sont d√©velopp√©s.

* Cr√©ation de la CNCF (Cloud Native Computing Foundation).

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

## Docker devient une plateforme

* Le moteur de conteneur initial est maintenant connu sous le nom de "Moteur Docker".

* D'autres outils sont ajout√©s:
  * Docker Compose (anciennement "Fig")
  * Machine Docker
  * Docker Swarm
  * Kitematic
  * Docker Cloud (anciennement "Tutum")
  * Datacenter Docker
  * etc.

* Docker Inc. lance des offres commerciales.

.debug[[intro/Docker_History_fr.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/intro/Docker_History_fr.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-our-sample-application
class: title

Our sample application

.nav[
[Previous section](#toc-histoire-des-conteneurs--et-docker)
|
[Back to table of contents](#toc-chapter-1)
|
[Next section](#toc-identifying-bottlenecks)
]

.debug[(automatically generated title slide)]

---
# Our sample application

- We will clone the GitHub repository onto our `node1`

- The repository also contains scripts and tools that we will use through the workshop

.exercise[

<!--
```bash
if [ -d container.training ]; then
  mv container.training container.training.$$
fi
```
-->

- Clone the repository on `node1`:
  ```bash
  git clone git://github.com/jpetazzo/container.training
  ```

]

(You can also fork the repository on GitHub and clone your fork if you prefer that.)

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

## Downloading and running the application

Let's start this before we look around, as downloading will take a little time...

.exercise[

- Go to the `dockercoins` directory, in the cloned repo:
  ```bash
  cd ~/container.training/dockercoins
  ```

- Use Compose to build and run all containers:
  ```bash
  docker-compose up
  ```

<!--
```longwait units of work done```
-->

]

Compose tells Docker to build all container images (pulling
the corresponding base images), then starts all containers,
and displays aggregated logs.

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

## More detail on our sample application

- Visit the GitHub repository with all the materials of this workshop:
  <br/>https://github.com/jpetazzo/container.training

- The application is in the [dockercoins](
  https://github.com/jpetazzo/container.training/tree/master/dockercoins)
  subdirectory

- Let's look at the general layout of the source code:

  there is a Compose file [docker-compose.yml](
  https://github.com/jpetazzo/container.training/blob/master/dockercoins/docker-compose.yml) ...

  ... and 4 other services, each in its own directory:

  - `rng` = web service generating random bytes
  - `hasher` = web service computing hash of POSTed data
  - `worker` = background process using `rng` and `hasher`
  - `webui` = web interface to watch progress

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

class: extra-details

## Compose file format version

*Particularly relevant if you have used Compose before...*

- Compose 1.6 introduced support for a new Compose file format (aka "v2")

- Services are no longer at the top level, but under a `services` section

- There has to be a `version` key at the top level, with value `"2"` (as a string, not an integer)

- Containers are placed on a dedicated network, making links unnecessary

- There are other minor differences, but upgrade is easy and straightforward

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

## Service discovery in container-land

- We do not hard-code IP addresses in the code

- We do not hard-code FQDN in the code, either

- We just connect to a service name, and container-magic does the rest

  (And by container-magic, we mean "a crafty, dynamic, embedded DNS server")

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

## Example in `worker/worker.py`

```python
redis = Redis("`redis`")


def get_random_bytes():
    r = requests.get("http://`rng`/32")
    return r.content


def hash_bytes(data):
    r = requests.post("http://`hasher`/",
                      data=data,
                      headers={"Content-Type": "application/octet-stream"})
```

(Full source code available [here](
https://github.com/jpetazzo/container.training/blob/8279a3bce9398f7c1a53bdd95187c53eda4e6435/dockercoins/worker/worker.py#L17
))

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

class: extra-details

## Links, naming, and service discovery

- Containers can have network aliases (resolvable through DNS)

- Compose file version 2+ makes each container reachable through its service name

- Compose file version 1 did require "links" sections

- Network aliases are automatically namespaced

  - you can have multiple apps declaring and using a service named `database`

  - containers in the blue app will resolve `database` to the IP of the blue database

  - containers in the green app will resolve `database` to the IP of the green database

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

## What's this application?

--

- It is a DockerCoin miner! .emoji[üí∞üê≥üì¶üö¢]

--

- No, you can't buy coffee with DockerCoins

--

- How DockerCoins works:

  - `worker` asks to `rng` to generate a few random bytes

  - `worker` feeds these bytes into `hasher`

  - and repeat forever!

  - every second, `worker` updates `redis` to indicate how many loops were done

  - `webui` queries `redis`, and computes and exposes "hashing speed" in your browser

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

## Our application at work

- On the left-hand side, the "rainbow strip" shows the container names

- On the right-hand side, we see the output of our containers

- We can see the `worker` service making requests to `rng` and `hasher`

- For `rng` and `hasher`, we see HTTP access logs

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

## Connecting to the web UI

- "Logs are exciting and fun!" (No-one, ever)

- The `webui` container exposes a web dashboard; let's view it

.exercise[

- With a web browser, connect to `node1` on port 8000

- Remember: the `nodeX` aliases are valid only on the nodes themselves

- In your browser, you need to enter the IP address of your node

<!-- ```open http://node1:8000``` -->

]

A drawing area should show up, and after a few seconds, a blue
graph will appear.

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

class: self-paced, extra-details

## If the graph doesn't load

If you just see a `Page not found` error, it might be because your
Docker Engine is running on a different machine. This can be the case if:

- you are using the Docker Toolbox

- you are using a VM (local or remote) created with Docker Machine

- you are controlling a remote Docker Engine

When you run DockerCoins in development mode, the web UI static files
are mapped to the container using a volume. Alas, volumes can only
work on a local environment, or when using Docker4Mac or Docker4Windows.

How to fix this?

Stop the app with `^C`, edit `dockercoins.yml`, comment out the `volumes` section, and try again.

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

class: extra-details

## Why does the speed seem irregular?

- It *looks like* the speed is approximately 4 hashes/second

- Or more precisely: 4 hashes/second, with regular dips down to zero

- Why?

--

class: extra-details

- The app actually has a constant, steady speed: 3.33 hashes/second
  <br/>
  (which corresponds to 1 hash every 0.3 seconds, for *reasons*)

- Yes, and?

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

class: extra-details

## The reason why this graph is *not awesome*

- The worker doesn't update the counter after every loop, but up to once per second

- The speed is computed by the browser, checking the counter about once per second

- Between two consecutive updates, the counter will increase either by 4, or by 0

- The perceived speed will therefore be 4 - 4 - 4 - 0 - 4 - 4 - 0 etc.

- What can we conclude from this?

--

class: extra-details

- "I'm clearly incapable of writing good frontend code!" üòÄ ‚Äî J√©r√¥me

.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---

## Stopping the application

- If we interrupt Compose (with `^C`), it will politely ask the Docker Engine to stop the app

- The Docker Engine will send a `TERM` signal to the containers

- If the containers do not exit in a timely manner, the Engine sends a `KILL` signal

.exercise[

- Stop the application by hitting `^C`

<!--
```keys ^C```
-->

]

--

Some containers exit immediately, others take longer.

The containers that do not handle `SIGTERM` end up being killed after a 10s timeout. If we are very impatient, we can hit `^C` a second time!


.debug[[common/sampleapp.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/sampleapp.md)]
---
## Restarting in the background

- Many flags and commands of Compose are modeled after those of `docker`

.exercise[

- Start the app in the background with the `-d` option:
  ```bash
  docker-compose up -d
  ```

- Check that our app is running with the `ps` command:
  ```bash
  docker-compose ps
  ```

]

`docker-compose ps` also shows the ports exposed by the application.

.debug[[common/composescale.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/composescale.md)]
---

class: extra-details

## Viewing logs

- The `docker-compose logs` command works like `docker logs`

.exercise[

- View all logs since container creation and exit when done:
  ```bash
  docker-compose logs
  ```

- Stream container logs, starting at the last 10 lines for each container:
  ```bash
  docker-compose logs --tail 10 --follow
  ```

<!--
```wait units of work done```
```keys ^C```
-->

]

Tip: use `^S` and `^Q` to pause/resume log output.

.debug[[common/composescale.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/composescale.md)]
---

## Scaling up the application

- Our goal is to make that performance graph go up (without changing a line of code!)

--

- Before trying to scale the application, we'll figure out if we need more resources

  (CPU, RAM...)

- For that, we will use good old UNIX tools on our Docker node

.debug[[common/composescale.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/composescale.md)]
---

## Looking at resource usage

- Let's look at CPU, memory, and I/O usage

.exercise[

- run `top` to see CPU and memory usage (you should see idle cycles)

<!--
```bash top```

```wait Tasks```
```keys ^C```
-->

- run `vmstat 1` to see I/O usage (si/so/bi/bo)
  <br/>(the 4 numbers should be almost zero, except `bo` for logging)

<!--
```bash vmstat 1```

```wait memory```
```keys ^C```
-->

]

We have available resources.

- Why?
- How can we use them?

.debug[[common/composescale.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/composescale.md)]
---

## Scaling workers on a single node

- Docker Compose supports scaling
- Let's scale `worker` and see what happens!

.exercise[

- Start one more `worker` container:
  ```bash
  docker-compose up --scale worker=2
  ```

- Look at the performance graph (it should show a x2 improvement)

- Look at the aggregated logs of our containers (`worker_2` should show up)

- Look at the impact on CPU load with e.g. top (it should be negligible)

]

.debug[[common/composescale.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/composescale.md)]
---

## Adding more workers

- Great, let's add more workers and call it a day, then!

.exercise[

- Start eight more `worker` containers:
  ```bash
  docker-compose up --scale worker=10
  ```

- Look at the performance graph: does it show a x10 improvement?

- Look at the aggregated logs of our containers

- Look at the impact on CPU load and memory usage

]

.debug[[common/composescale.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/composescale.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-identifying-bottlenecks
class: title

Identifying bottlenecks

.nav[
[Previous section](#toc-our-sample-application)
|
[Back to table of contents](#toc-chapter-1)
|
[Next section](#toc-kubernetes-concepts)
]

.debug[(automatically generated title slide)]

---

# Identifying bottlenecks

- You should have seen a 3x speed bump (not 10x)

- Adding workers didn't result in linear improvement

- *Something else* is slowing us down

--

- ... But what?

--

- The code doesn't have instrumentation

- Let's use state-of-the-art HTTP performance analysis!
  <br/>(i.e. good old tools like `ab`, `httping`...)

.debug[[common/composescale.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/composescale.md)]
---

## Accessing internal services

- `rng` and `hasher` are exposed on ports 8001 and 8002

- This is declared in the Compose file:

  ```yaml
    ...
    rng:
      build: rng
      ports:
      - "8001:80"

    hasher:
      build: hasher
      ports:
      - "8002:80"
    ...
  ```

.debug[[common/composescale.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/composescale.md)]
---

## Measuring latency under load

We will use `httping`.

.exercise[

- Check the latency of `rng`:
  ```bash
  httping -c 3 localhost:8001
  ```

- Check the latency of `hasher`:
  ```bash
  httping -c 3 localhost:8002
  ```

]

`rng` has a much higher latency than `hasher`.

.debug[[common/composescale.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/composescale.md)]
---

## Let's draw hasty conclusions

- The bottleneck seems to be `rng`

- *What if* we don't have enough entropy and can't generate enough random numbers?

- We need to scale out the `rng` service on multiple machines!

Note: this is a fiction! We have enough entropy. But we need a pretext to scale out.

(In fact, the code of `rng` uses `/dev/urandom`, which never runs out of entropy...
<br/>
...and is [just as good as `/dev/random`](http://www.slideshare.net/PacSecJP/filippo-plain-simple-reality-of-entropy).)

.debug[[common/composescale.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/composescale.md)]
---
## Clean up

- Before moving on, let's remove those containers

.exercise[

- Tell Compose to remove everything:
  ```bash
  docker-compose down
  ```

]

.debug[[common/composedown.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/composedown.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-kubernetes-concepts
class: title

Kubernetes concepts

.nav[
[Previous section](#toc-identifying-bottlenecks)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-declarative-vs-imperative)
]

.debug[(automatically generated title slide)]

---
# Kubernetes concepts

- Kubernetes is a container management system

- It runs and manages containerized applications on a cluster

--

- What does that really mean?

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Basic things we can ask Kubernetes to do

--

- Start 5 containers using image `atseashop/api:v1.3`

--

- Place an internal load balancer in front of these containers

--

- Start 10 containers using image `atseashop/webfront:v1.3`

--

- Place a public load balancer in front of these containers

--

- It's Black Friday (or Christmas), traffic spikes, grow our cluster and add containers

--

- New release! Replace my containers with the new image `atseashop/webfront:v1.4`

--

- Keep processing requests during the upgrade; update my containers one at a time

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Other things that Kubernetes can do for us

- Basic autoscaling

- Blue/green deployment, canary deployment

- Long running services, but also batch (one-off) jobs

- Overcommit our cluster and *evict* low-priority jobs

- Run services with *stateful* data (databases etc.)

- Fine-grained access control defining *what* can be done by *whom* on *which* resources

- Integrating third party services (*service catalog*)

- Automating complex tasks (*operators*)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Kubernetes architecture

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

class: pic

![haha only kidding](images/k8s-arch1.png)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Kubernetes architecture

- Ha ha ha ha

- OK, I was trying to scare you, it's much simpler than that ‚ù§Ô∏è

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

class: pic

![that one is more like the real thing](images/k8s-arch2.png)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Credits

- The first schema is a Kubernetes cluster with storage backed by multi-path iSCSI

  (Courtesy of [Yongbok Kim](https://www.yongbok.net/blog/))

- The second one is a simplified representation of a Kubernetes cluster

  (Courtesy of [Imesh Gunaratne](https://medium.com/containermind/a-reference-architecture-for-deploying-wso2-middleware-on-kubernetes-d4dee7601e8e))

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Kubernetes architecture: the nodes

- The nodes executing our containers run a collection of services:

  - a container Engine (typically Docker)

  - kubelet (the "node agent")

  - kube-proxy (a necessary but not sufficient network component)

- Nodes were formerly called "minions"

  (You might see that word in older articles or documentation)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Kubernetes architecture: the control plane

- The Kubernetes logic (its "brains") is a collection of services:

  - the API server (our point of entry to everything!)

  - core services like the scheduler and controller manager

  - `etcd` (a highly available key/value store; the "database" of Kubernetes)

- Together, these services form the control plane of our cluster

- The control plane is also called the "master"

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Running the control plane on special nodes

- It is common to reserve a dedicated node for the control plane

  (Except for single-node development clusters, like when using minikube)

- This node is then called a "master"

  (Yes, this is ambiguous: is the "master" a node, or the whole control plane?)

- Normal applications are restricted from running on this node

  (By using a mechanism called ["taints"](https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/))

- When high availability is required, each service of the control plane must be resilient

- The control plane is then replicated on multiple nodes

  (This is sometimes called a "multi-master" setup)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Running the control plane outside containers

- The services of the control plane can run in or out of containers

- For instance: since `etcd` is a critical service, some people
  deploy it directly on a dedicated cluster (without containers)

  (This is illustrated on the first "super complicated" schema)

- In some hosted Kubernetes offerings (e.g. GKE), the control plane is invisible

  (We only "see" a Kubernetes API endpoint)

- In that case, there is no "master node"

*For this reason, it is more accurate to say "control plane" rather than "master".*

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Do we need to run Docker at all?

No!

--

- By default, Kubernetes uses the Docker Engine to run containers

- We could also use `rkt` ("Rocket") from CoreOS

- Or leverage other pluggable runtimes through the *Container Runtime Interface*

  (like CRI-O, or containerd)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Do we need to run Docker at all?

Yes!

--

- In this workshop, we run our app on a single node first

- We will need to build images and ship them around

- We can do these things without Docker
  <br/>
  (and get diagnosed with NIH¬π syndrome)

- Docker is still the most stable container engine today
  <br/>
  (but other options are maturing very quickly)

.footnote[¬π[Not Invented Here](https://en.wikipedia.org/wiki/Not_invented_here)]

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Do we need to run Docker at all?

- On our development environments, CI pipelines ... :

  *Yes, almost certainly*

- On our production servers:

  *Yes (today)*

  *Probably not (in the future)*

.footnote[More information about CRI [on the Kubernetes blog](https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes)]

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Kubernetes resources

- The Kubernetes API defines a lot of objects called *resources*

- These resources are organized by type, or `Kind` (in the API)

- A few common resource types are:

  - node (a machine ‚Äî physical or virtual ‚Äî in our cluster)
  - pod (group of containers running together on a node)
  - service (stable network endpoint to connect to one or multiple containers)
  - namespace (more-or-less isolated group of things)
  - secret (bundle of sensitive data to be passed to a container)
 
  And much more! (We can see the full list by running `kubectl get`)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

class: pic

![Node, pod, container](images/k8s-arch3-thanks-weave.png)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

class: pic

![One of the best Kubernetes architecture diagrams available](images/k8s-arch4-thanks-luxas.png)

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

## Credits

- The first diagram is courtesy of Weave Works

  - a *pod* can have multiple containers working together

  - IP addresses are associated with *pods*, not with individual containers

- The second diagram is courtesy of Lucas K√§ldstr√∂m, in [this presentation](https://speakerdeck.com/luxas/kubeadm-cluster-creation-internals-from-self-hosting-to-upgradability-and-ha)

  - it's one of the best Kubernetes architecture diagrams available!

Both diagrams used with permission.

.debug[[kube/concepts-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/concepts-k8s.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-declarative-vs-imperative
class: title

Declarative vs imperative

.nav[
[Previous section](#toc-kubernetes-concepts)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-kubernetes-network-model)
]

.debug[(automatically generated title slide)]

---
# Declarative vs imperative

- Our container orchestrator puts a very strong emphasis on being *declarative*

- Declarative:

  *I would like a cup of tea.*

- Imperative:

  *Boil some water. Pour it in a teapot. Add tea leaves. Steep for a while. Serve in a cup.*

--

- Declarative seems simpler at first ... 

--

- ... As long as you know how to brew tea

.debug[[common/declarative.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/declarative.md)]
---

## Declarative vs imperative

- What declarative would really be:

  *I want a cup of tea, obtained by pouring an infusion¬π of tea leaves in a cup.*

--

  *¬πAn infusion is obtained by letting the object steep a few minutes in hot¬≤ water.*

--

  *¬≤Hot liquid is obtained by pouring it in an appropriate container¬≥ and setting it on a stove.*

--

  *¬≥Ah, finally, containers! Something we know about. Let's get to work, shall we?*

--

.footnote[Did you know there was an [ISO standard](https://en.wikipedia.org/wiki/ISO_3103)
specifying how to brew tea?]

.debug[[common/declarative.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/declarative.md)]
---

## Declarative vs imperative

- Imperative systems:

  - simpler

  - if a task is interrupted, we have to restart from scratch

- Declarative systems:

  - if a task is interrupted (or if we show up to the party half-way through),
    we can figure out what's missing and do only what's necessary

  - we need to be able to *observe* the system

  - ... and compute a "diff" between *what we have* and *what we want*

.debug[[common/declarative.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/common/declarative.md)]
---
## Declarative vs imperative in Kubernetes

- Virtually everything we create in Kubernetes is created from a *spec*

- Watch for the `spec` fields in the YAML files later!

- The *spec* describes *how we want the thing to be*

- Kubernetes will *reconcile* the current state with the spec
  <br/>(technically, this is done by a number of *controllers*)

- When we want to change some resource, we update the *spec*

- Kubernetes will then *converge* that resource

.debug[[kube/declarative.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/declarative.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/containers-by-the-water.jpg)]

---

name: toc-kubernetes-network-model
class: title

Kubernetes network model

.nav[
[Previous section](#toc-declarative-vs-imperative)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-first-contact-with-kubectl)
]

.debug[(automatically generated title slide)]

---
# Kubernetes network model

- TL,DR:

  *Our cluster (nodes and pods) is one big flat IP network.*

--

- In detail:

 - all nodes must be able to reach each other, without NAT

 - all pods must be able to reach each other, without NAT

 - pods and nodes must be able to reach each other, without NAT

 - each pod is aware of its IP address (no NAT)

- Kubernetes doesn't mandate any particular implementation

.debug[[kube/kubenet.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubenet.md)]
---

## Kubernetes network model: the good

- Everything can reach everything

- No address translation

- No port translation

- No new protocol

- Pods cannot move from a node to another and keep their IP address

- IP addresses don't have to be "portable" from a node to another

  (We can use e.g. a subnet per node and use a simple routed topology)

- The specification is simple enough to allow many various implementations

.debug[[kube/kubenet.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubenet.md)]
---

## Kubernetes network model: the less good

- Everything can reach everything

  - if you want security, you need to add network policies

  - the network implementation that you use needs to support them

- There are literally dozens of implementations out there

  (15 are listed in the Kubernetes documentation)

- Pods have level 3 (IP) connectivity, but *services* are level 4

  (Services map to a single UDP or TCP port; no port ranges or arbitrary IP packets)

- `kube-proxy` is on the data path when connecting to a pod or container,
  <br/>and it's not particularly fast (relies on userland proxying or iptables)

.debug[[kube/kubenet.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubenet.md)]
---

## Kubernetes network model: in practice

- The nodes that we are using have been set up to use [Weave](https://github.com/weaveworks/weave)

- We don't endorse Weave in a particular way, it just Works For Us

- Don't worry about the warning about `kube-proxy` performance

- Unless you:

  - routinely saturate 10G network interfaces
  - count packet rates in millions per second
  - run high-traffic VOIP or gaming platforms
  - do weird things that involve millions of simultaneous connections
    <br/>(in which case you're already familiar with kernel tuning)

- If necessary, there are alternatives to `kube-proxy`; e.g.
  [`kube-router`](https://www.kube-router.io)

.debug[[kube/kubenet.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubenet.md)]
---

## The Container Network Interface (CNI)

- The CNI has a well-defined [specification](https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration) for network plugins

- When a pod is created, Kubernetes delegates the network setup to CNI plugins

- Typically, a CNI plugin will:

  - allocate an IP address (by calling an IPAM plugin)

  - add a network interface into the pod's network namespace

  - configure the interface as well as required routes etc.

- Using multiple plugins can be done with "meta-plugins" like CNI-Genie or Multus

- Not all CNI plugins are equal

  (e.g. they don't all implement network policies, which are required to isolate pods)

.debug[[kube/kubenet.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubenet.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/distillery-containers.jpg)]

---

name: toc-first-contact-with-kubectl
class: title

First contact with `kubectl`

.nav[
[Previous section](#toc-kubernetes-network-model)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-setting-up-kubernetes)
]

.debug[(automatically generated title slide)]

---
# First contact with `kubectl`

- `kubectl` is (almost) the only tool we'll need to talk to Kubernetes

- It is a rich CLI tool around the Kubernetes API

  (Everything you can do with `kubectl`, you can do directly with the API)

- On our machines, there is a `~/.kube/config` file with:

  - the Kubernetes API address

  - the path to our TLS certificates used to authenticate

- You can also use the `--kubeconfig` flag to pass a config file

- Or directly `--server`, `--user`, etc.

- `kubectl` can be pronounced "Cube C T L", "Cube cuttle", "Cube cuddle"...

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

## `kubectl get`

- Let's look at our `Node` resources with `kubectl get`!

.exercise[

- Look at the composition of our cluster:
  ```bash
  kubectl get node
  ```

- These commands are equivalent:
  ```bash
  kubectl get no
  kubectl get node
  kubectl get nodes
  ```

]

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

## Obtaining machine-readable output

- `kubectl get` can output JSON, YAML, or be directly formatted

.exercise[

- Give us more info about the nodes:
  ```bash
  kubectl get nodes -o wide
  ```

- Let's have some YAML:
  ```bash
  kubectl get no -o yaml
  ```
  See that `kind: List` at the end? It's the type of our result!

]

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

## (Ab)using `kubectl` and `jq`

- It's super easy to build custom reports

.exercise[

- Show the capacity of all our nodes as a stream of JSON objects:
  ```bash
    kubectl get nodes -o json | 
            jq ".items[] | {name:.metadata.name} + .status.capacity"
  ```

]

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

## What's available?

- `kubectl` has pretty good introspection facilities

- We can list all available resource types by running `kubectl get`

- We can view details about a resource with:
  ```bash
  kubectl describe type/name
  kubectl describe type name
  ```

- We can view the definition for a resource type with:
  ```bash
  kubectl explain type
  ```

Each time, `type` can be singular, plural, or abbreviated type name.

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

## Services

- A *service* is a stable endpoint to connect to "something"

  (In the initial proposal, they were called "portals")

.exercise[

- List the services on our cluster with one of these commands:
  ```bash
  kubectl get services
  kubectl get svc
  ```

]

--

There is already one service on our cluster: the Kubernetes API itself.

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

## ClusterIP services

- A `ClusterIP` service is internal, available from the cluster only

- This is useful for introspection from within containers

.exercise[

- Try to connect to the API:
  ```bash
  curl -k https://`10.96.0.1`
  ```
  
  - `-k` is used to skip certificate verification

  - Make sure to replace 10.96.0.1 with the CLUSTER-IP shown by `kubectl get svc`

]

--

The error that we see is expected: the Kubernetes API requires authentication.

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

## Listing running containers

- Containers are manipulated through *pods*

- A pod is a group of containers:

 - running together (on the same node)

 - sharing resources (RAM, CPU; but also network, volumes)

.exercise[

- List pods on our cluster:
  ```bash
  kubectl get pods
  ```

]

--

*These are not the pods you're looking for.* But where are they?!?

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

## Namespaces

- Namespaces allow us to segregate resources

.exercise[

- List the namespaces on our cluster with one of these commands:
  ```bash
  kubectl get namespaces
  kubectl get namespace
  kubectl get ns
  ```

]

--

*You know what ... This `kube-system` thing looks suspicious.*

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

## Accessing namespaces

- By default, `kubectl` uses the `default` namespace

- We can switch to a different namespace with the `-n` option

.exercise[

- List the pods in the `kube-system` namespace:
  ```bash
  kubectl -n kube-system get pods
  ```

]

--

*Ding ding ding ding ding!*

The `kube-system` namespace is used for the control plane.

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

## What are all these control plane pods?

- `etcd` is our etcd server

- `kube-apiserver` is the API server

- `kube-controller-manager` and `kube-scheduler` are other master components

- `kube-dns` is an additional component (not mandatory but super useful, so it's there)

- `kube-proxy` is the (per-node) component managing port mappings and such

- `weave` is the (per-node) component managing the network overlay

- the `READY` column indicates the number of containers in each pod

- the pods with a name ending with `-node1` are the master components
  <br/>
  (they have been specifically "pinned" to the master node)

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

## What about `kube-public`?

.exercise[

- List the pods in the `kube-public` namespace:
  ```bash
  kubectl -n kube-public get pods
  ```

]

--

- Maybe it doesn't have pods, but what secrets is `kube-public` keeping?

--

.exercise[

- List the secrets in the `kube-public` namespace:
  ```bash
  kubectl -n kube-public get secrets
  ```

]
--

- `kube-public` is created by kubeadm & [used for security bootstrapping](https://kubernetes.io/blog/2017/01/stronger-foundation-for-creating-and-managing-kubernetes-clusters)

.debug[[kube/kubectlget.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlget.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/lots-of-containers.jpg)]

---

name: toc-setting-up-kubernetes
class: title

Setting up Kubernetes

.nav[
[Previous section](#toc-first-contact-with-kubectl)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-running-our-first-containers-on-kubernetes)
]

.debug[(automatically generated title slide)]

---
# Setting up Kubernetes

- How did we set up these Kubernetes clusters that we're using?

--

- We used `kubeadm` on freshly installed VM instances running Ubuntu 16.04 LTS

    1. Install Docker

    2. Install Kubernetes packages

    3. Run `kubeadm init` on the master node

    4. Set up Weave (the overlay network)
       <br/>
       (that step is just one `kubectl apply` command; discussed later)

    5. Run `kubeadm join` on the other nodes (with the token produced by `kubeadm init`)

    6. Copy the configuration file generated by `kubeadm init`

- Check the [prepare VMs README](https://github.com/jpetazzo/container.training/blob/master/prepare-vms/README.md) for more details

.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/setup-k8s.md)]
---

## `kubeadm` drawbacks

- Doesn't set up Docker or any other container engine

- Doesn't set up the overlay network

- Doesn't set up multi-master (no high availability)

--

  (At least ... not yet!)

--

- "It's still twice as many steps as setting up a Swarm cluster üòï" -- J√©r√¥me

.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/setup-k8s.md)]
---

## Other deployment options

- If you are on Azure:
  [AKS](https://azure.microsoft.com/services/container-service/)

- If you are on Google Cloud:
  [GKE](https://cloud.google.com/kubernetes-engine/)

- If you are on AWS:
  [EKS](https://aws.amazon.com/eks/)
  or
  [kops](https://github.com/kubernetes/kops)

- On a local machine:
  [minikube](https://kubernetes.io/docs/getting-started-guides/minikube/),
  [kubespawn](https://github.com/kinvolk/kube-spawn),
  [Docker4Mac](https://docs.docker.com/docker-for-mac/kubernetes/)

- If you want something customizable:
  [kubicorn](https://github.com/kubicorn/kubicorn)

  Probably the closest to a multi-cloud/hybrid solution so far, but in development

.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/setup-k8s.md)]
---

## Even more deployment options

- If you like Ansible:
  [kubespray](https://github.com/kubernetes-incubator/kubespray)

- If you like Terraform:
  [typhoon](https://github.com/poseidon/typhoon/)

- You can also learn how to install every component manually, with
  the excellent tutorial [Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way)

  *Kubernetes The Hard Way is optimized for learning, which means taking the long route to ensure you understand each task required to bootstrap a Kubernetes cluster.*

- There are also many commercial options available!

- For a longer list, check the Kubernetes documentation:
  <br/>
  it has a great guide to [pick the right solution](https://kubernetes.io/docs/setup/pick-right-solution/) to set up Kubernetes.

.debug[[kube/setup-k8s.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/setup-k8s.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/plastic-containers.JPG)]

---

name: toc-running-our-first-containers-on-kubernetes
class: title

Running our first containers on Kubernetes

.nav[
[Previous section](#toc-setting-up-kubernetes)
|
[Back to table of contents](#toc-chapter-2)
|
[Next section](#toc-the-kubernetes-dashboard)
]

.debug[(automatically generated title slide)]

---
# Running our first containers on Kubernetes

- First things first: we cannot run a container

--

- We are going to run a pod, and in that pod there will be a single container

--

- In that container in the pod, we are going to run a simple `ping` command

- Then we are going to start additional copies of the pod

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

## Starting a simple pod with `kubectl run`

- We need to specify at least a *name* and the image we want to use

.exercise[

- Let's ping `1.1.1.1`, Cloudflare's 
  [public DNS resolver](https://blog.cloudflare.com/announcing-1111/):
  ```bash
  kubectl run pingpong --image alpine ping 1.1.1.1
  ```

]

--

OK, what just happened?

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

## Behind the scenes of `kubectl run`

- Let's look at the resources that were created by `kubectl run`

.exercise[

- List most resource types:
  ```bash
  kubectl get all
  ```

]

--

We should see the following things:
- `deployment.apps/pingpong` (the *deployment* that we just created)
- `replicaset.apps/pingpong-xxxxxxxxxx` (a *replica set* created by the deployment)
- `pod/pingpong-xxxxxxxxxx-yyyyy` (a *pod* created by the replica set)

Note: as of 1.10.1, resource types are displayed in more detail.

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

## What are these different things?

- A *deployment* is a high-level construct

  - allows scaling, rolling updates, rollbacks

  - multiple deployments can be used together to implement a
    [canary deployment](https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments)

  - delegates pods management to *replica sets*

- A *replica set* is a low-level construct

  - makes sure that a given number of identical pods are running

  - allows scaling

  - rarely used directly

- A *replication controller* is the (deprecated) predecessor of a replica set

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

## Our `pingpong` deployment

- `kubectl run` created a *deployment*, `deployment.apps/pingpong`

```
NAME                       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/pingpong   1         1         1            1           10m
```

- That deployment created a *replica set*, `replicaset.apps/pingpong-xxxxxxxxxx`

```
NAME                                  DESIRED   CURRENT   READY     AGE
replicaset.apps/pingpong-7c8bbcd9bc   1         1         1         10m
```

- That replica set created a *pod*, `pod/pingpong-xxxxxxxxxx-yyyyy`

```
NAME                            READY     STATUS    RESTARTS   AGE
pod/pingpong-7c8bbcd9bc-6c9qz   1/1       Running   0          10m
```

- We'll see later how these folks play together for:

  - scaling, high availability, rolling updates

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

## Viewing container output

- Let's use the `kubectl logs` command

- We will pass either a *pod name*, or a *type/name*

  (E.g. if we specify a deployment or replica set, it will get the first pod in it)

- Unless specified otherwise, it will only show logs of the first container in the pod

  (Good thing there's only one in ours!)

.exercise[

- View the result of our `ping` command:
  ```bash
  kubectl logs deploy/pingpong
  ```

]

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

## Streaming logs in real time

- Just like `docker logs`, `kubectl logs` supports convenient options:

  - `-f`/`--follow` to stream logs in real time (√† la `tail -f`)

  - `--tail` to indicate how many lines you want to see (from the end)

  - `--since` to get logs only after a given timestamp

.exercise[

- View the latest logs of our `ping` command:
  ```bash
  kubectl logs deploy/pingpong --tail 1 --follow
  ```

<!--
```wait seq=3```
```keys ^C```
-->

]

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

## Scaling our application

- We can create additional copies of our container (I mean, our pod) with `kubectl scale`

.exercise[

- Scale our `pingpong` deployment:
  ```bash
  kubectl scale deploy/pingpong --replicas 8
  ```

]

Note: what if we tried to scale `replicaset.apps/pingpong-xxxxxxxxxx`?

We could! But the *deployment* would notice it right away, and scale back to the initial level.

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

## Resilience

- The *deployment* `pingpong` watches its *replica set*

- The *replica set* ensures that the right number of *pods* are running

- What happens if pods disappear?

.exercise[

- In a separate window, list pods, and keep watching them:
  ```bash
  kubectl get pods -w
  ```

<!--
```wait Running```
```keys ^C```
-->

- Destroy a pod:
  ```bash
  kubectl delete pod pingpong-xxxxxxxxxx-yyyyy
  ```
]

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

## What if we wanted something different?

- What if we wanted to start a "one-shot" container that *doesn't* get restarted?

- We could use `kubectl run --restart=OnFailure` or `kubectl run --restart=Never`

- These commands would create *jobs* or *pods* instead of *deployments*

- Under the hood, `kubectl run` invokes "generators" to create resource descriptions

- We could also write these resource descriptions ourselves (typically in YAML),
  <br/>and create them on the cluster with `kubectl apply -f` (discussed later)

- With `kubectl run --schedule=...`, we can also create *cronjobs*

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

## Viewing logs of multiple pods

- When we specify a deployment name, only one single pod's logs are shown

- We can view the logs of multiple pods by specifying a *selector*

- A selector is a logic expression using *labels*

- Conveniently, when you `kubectl run somename`, the associated objects have a `run=somename` label

.exercise[

- View the last line of log from all pods with the `run=pingpong` label:
  ```bash
  kubectl logs -l run=pingpong --tail 1
  ```

]

Unfortunately, `--follow` cannot (yet) be used to stream the logs from multiple containers.

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

## Aren't we flooding 1.1.1.1?

- If you're wondering this, good question!

- Don't worry, though:

  *APNIC's research group held the IP addresses 1.1.1.1 and 1.0.0.1. While the addresses were valid, so many people had entered them into various random systems that they were continuously overwhelmed by a flood of garbage traffic. APNIC wanted to study this garbage traffic but any time they'd tried to announce the IPs, the flood would overwhelm any conventional network.*

  (Source: https://blog.cloudflare.com/announcing-1111/)

- It's very unlikely that our concerted pings manage to produce
  even a modest blip at Cloudflare's NOC!

.debug[[kube/kubectlrun.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/kubectlrun.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-1.jpg)]

---

name: toc-the-kubernetes-dashboard
class: title

The Kubernetes dashboard

.nav[
[Previous section](#toc-running-our-first-containers-on-kubernetes)
|
[Back to table of contents](#toc-chapter-3)
|
[Next section](#toc-security-implications-of-kubectl-apply)
]

.debug[(automatically generated title slide)]

---
# The Kubernetes dashboard

- Kubernetes resources can also be viewed with a web dashboard

- We are going to deploy that dashboard with *three commands:*

  1) actually *run* the dashboard

  2) bypass SSL for the dashboard

  3) bypass authentication for the dashboard

--

There is an additional step to make the dashboard available from outside (we'll get to that)

--

.footnote[.warning[Yes, this will open our cluster to all kinds of shenanigans. Don't do this at home.]]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]
---

## 1) Running the dashboard

- We need to create a *deployment* and a *service* for the dashboard

- But also a *secret*, a *service account*, a *role* and a *role binding*

- All these things can be defined in a YAML file and created with `kubectl apply -f`

.exercise[

- Create all the dashboard resources, with the following command:
  ```bash
  kubectl apply -f https://goo.gl/Qamqab
  ```

]

The goo.gl URL expands to:
<br/>
.small[https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]
---


## 2) Bypassing SSL for the dashboard

- The Kubernetes dashboard uses HTTPS, but we don't have a certificate

- Recent versions of Chrome (63 and later) and Edge will refuse to connect

  (You won't even get the option to ignore a security warning!)

- We could (and should!) get a certificate, e.g. with [Let's Encrypt](https://letsencrypt.org/)

- ... But for convenience, for this workshop, we'll forward HTTP to HTTPS

.warning[Do not do this at home, or even worse, at work!]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]
---

## Running the SSL unwrapper

- We are going to run [`socat`](http://www.dest-unreach.org/socat/doc/socat.html), telling it to accept TCP connections and relay them over SSL

- Then we will expose that `socat` instance with a `NodePort` service

- For convenience, these steps are neatly encapsulated into another YAML file

.exercise[

- Apply the convenient YAML file, and defeat SSL protection:
  ```bash
  kubectl apply -f https://goo.gl/tA7GLz
  ```

]

The goo.gl URL expands to:
<br/>
.small[.small[https://gist.githubusercontent.com/jpetazzo/c53a28b5b7fdae88bc3c5f0945552c04/raw/da13ef1bdd38cc0e90b7a4074be8d6a0215e1a65/socat.yaml]]

.warning[All our dashboard traffic is now clear-text, including passwords!]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]
---

## Connecting to the dashboard

.exercise[

- Check which port the dashboard is on:
  ```bash
  kubectl -n kube-system get svc socat
  ```

]

You'll want the `3xxxx` port.


.exercise[

- Connect to http://oneofournodes:3xxxx/

<!-- ```open https://node1:3xxxx/``` -->

]

The dashboard will then ask you which authentication you want to use.

.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]
---

## Dashboard authentication

- We have three authentication options at this point:

  - token (associated with a role that has appropriate permissions)

  - kubeconfig (e.g. using the `~/.kube/config` file from `node1`)

  - "skip" (use the dashboard "service account")

- Let's use "skip": we get a bunch of warnings and don't see much

.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]
---

## 3) Bypass authentication for the dashboard

- The dashboard documentation [explains how to do this](https://github.com/kubernetes/dashboard/wiki/Access-control#admin-privileges)

- We just need to load another YAML file!

.exercise[

- Grant admin privileges to the dashboard so we can see our resources:
  ```bash
  kubectl apply -f https://goo.gl/CHsLTA
  ```

- Reload the dashboard and enjoy!

]

--

.warning[By the way, we just added a backdoor to our Kubernetes cluster!]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]
---

## Exposing the dashboard over HTTPS

- We took a shortcut by forwarding HTTP to HTTPS inside the cluster

- Let's expose the dashboard over HTTPS!

- The dashboard is exposed through a `ClusterIP` service (internal traffic only)

- We will change that into a `NodePort` service (accepting outside traffic)

.exercise[

- Edit the service:
  ```bash
  kubectl edit service kubernetes-dashboard
  ```

]

--

`NotFound`?!? Y U NO WORK?!?

.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]
---

## Editing the `kubernetes-dashboard` service

- If we look at the [YAML](https://goo.gl/Qamqab) that we loaded before, we'll get a hint

--

- The dashboard was created in the `kube-system` namespace

--

.exercise[

- Edit the service:
  ```bash
  kubectl -n kube-system edit service kubernetes-dashboard
  ```

- Change `ClusterIP` to `NodePort`, save, and exit

- Check the port that was assigned with `kubectl -n kube-system get services`

- Connect to https://oneofournodes:3xxxx/ (yes, https)

]

.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]
---

## Running the Kubernetes dashboard securely

- The steps that we just showed you are *for educational purposes only!*

- If you do that on your production cluster, people [can and will abuse it](https://blog.redlock.io/cryptojacking-tesla)

- For an in-depth discussion about securing the dashboard,
  <br/>
  check [this excellent post on Heptio's blog](https://blog.heptio.com/on-securing-the-kubernetes-dashboard-16b09b1b7aca)

.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]
---

class: pic

.interstitial[![Image separating from the next chapter](https://gallant-turing-d0d520.netlify.com/containers/train-of-containers-2.jpg)]

---

name: toc-security-implications-of-kubectl-apply
class: title

Security implications of `kubectl apply`

.nav[
[Previous section](#toc-the-kubernetes-dashboard)
|
[Back to table of contents](#toc-chapter-3)
|
[Next section](#toc-)
]

.debug[(automatically generated title slide)]

---

# Security implications of `kubectl apply`

- When we do `kubectl apply -f <URL>`, we create arbitrary resources

- Resources can be evil; imagine a `deployment` that ...

--

  - starts bitcoin miners on the whole cluster

--

  - hides in a non-default namespace

--

  - bind-mounts our nodes' filesystem

--

  - inserts SSH keys in the root account (on the node)

--

  - encrypts our data and ransoms it

--

  - ‚ò†Ô∏è‚ò†Ô∏è‚ò†Ô∏è

.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]
---

## `kubectl apply` is the new `curl | sh`

- `curl | sh` is convenient

- It's safe if you use HTTPS URLs from trusted sources

--

- `kubectl apply -f` is convenient

- It's safe if you use HTTPS URLs from trusted sources

- Example: the official setup instructions for most pod networks

--

- It introduces new failure modes (like if you try to apply yaml from a link that's no longer valid)


.debug[[kube/dashboard.md](https://github.com/RyaxTech/container.training.git/tree/master/slides/kube/dashboard.md)]</textarea>
    <script src="remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true,
        excludedClasses: ["self-paced"]
      });
    </script>
    
    <!-- 
    These two scripts will be available only when loading the
    content using the pub/sub server. Otherwise, they'll just
    404 and that's OK.
    -->
    <script src="/socket.io/socket.io.js">
    </script>
    <script src="/remote.js">
    </script>

  </body>
</html>
